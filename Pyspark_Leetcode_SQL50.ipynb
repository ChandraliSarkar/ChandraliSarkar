{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNKrtjhp33gl5whODWElXDB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChandraliSarkar/ChandraliSarkar/blob/main/Pyspark_Leetcode_SQL50.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RjaXdr3xjmJF",
        "outputId": "9e2f8525-f5bd-4948-deb2-1a04d9f7c6ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=7186d48e39ae587b1f70139876b46cac9f70a94fdf724ed78d529256cc8eaeb6\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"PySparkExample_1757\") \\\n",
        "    .getOrCreate()\n",
        "data = [(0, 'Y', 'N'),\n",
        "        (1, 'Y', 'Y'),\n",
        "        (2, 'N', 'Y'),\n",
        "        (3, 'Y', 'Y'),\n",
        "        (4, 'N', 'N')]\n",
        "# Define the schema\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
        "schema = StructType([\n",
        "    StructField(\"product_id\", IntegerType(), True),\n",
        "    StructField(\"low_fats\", StringType(), True),\n",
        "    StructField(\"recyclable\", StringType(), True)\n",
        "])\n",
        "\n",
        "# Create DataFrame\n",
        "df = spark.createDataFrame(data, schema)\n",
        "\n",
        "# Filter the DataFrame to select product_ids where both low_fats='Y' and recyclable='Y'\n",
        "result_df = df.filter((df.low_fats == 'Y') & (df.recyclable == 'Y')).select(\"product_id\")\n",
        "\n",
        "# Show the result\n",
        "result_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2qevpckAleeC",
        "outputId": "41c8c509-29cd-4d3a-f97b-14dd90f0bb1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+\n",
            "|product_id|\n",
            "+----------+\n",
            "|         1|\n",
            "|         3|\n",
            "+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Leetcode_584\") \\\n",
        "    .getOrCreate()\n",
        "data = [\n",
        "    (1, 'Will', None),\n",
        "    (2, 'Jane', None),\n",
        "    (3, 'Alex', 2),\n",
        "    (4, 'Bill', None),\n",
        "    (5, 'Zack', 1),\n",
        "    (6, 'Mark', 2)\n",
        "]\n",
        "\n",
        "# Define the schema\n",
        "schema = [\"id\", \"name\", \"referee_id\"]\n",
        "\n",
        "# Create DataFrame\n",
        "df = spark.createDataFrame(data, schema)\n",
        "\n",
        "# Filter the DataFrame to exclude customers referred by the customer with id = 2\n",
        "result_df = df.filter((df.referee_id != 2) | (df.referee_id.isNull())).select(\"name\")\n",
        "\n",
        "# Select the 'name' column\n",
        "#result_df = result_df.select(\"name\")\n",
        "\n",
        "# Show the result\n",
        "result_df.show()"
      ],
      "metadata": {
        "id": "DnY4fmnhlweQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6907ef59-4178-4ddc-af9a-8d410447f3f7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+\n",
            "|name|\n",
            "+----+\n",
            "|Will|\n",
            "|Jane|\n",
            "|Bill|\n",
            "|Zack|\n",
            "+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AVT4vY5Qs6WV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder.appName(\"BigCountriesQuery\").getOrCreate()\n",
        "\n",
        "# Sample data\n",
        "data = [\n",
        "    (\"Afghanistan\", \"Asia\", 652230, 25500100, 20343000000),\n",
        "    (\"Albania\", \"Europe\", 28748, 2831741, 12960000000),\n",
        "    (\"Algeria\", \"Africa\", 2381741, 37100000, 188681000000),\n",
        "    (\"Andorra\", \"Europe\", 468, 78115, 3712000000),\n",
        "    (\"Angola\", \"Africa\", 1246700, 20609294, 100990000000)\n",
        "]\n",
        "\n",
        "# Define the schema\n",
        "schema = [\"name\", \"continent\", \"area\", \"population\", \"gdp\"]\n",
        "\n",
        "# Create DataFrame\n",
        "df = spark.createDataFrame(data, schema)\n",
        "\n",
        "# Filter the DataFrame to find big countries\n",
        "big_countries_df = df.filter((col(\"area\") >= 3000000) | (col(\"population\") >= 25000000))\n",
        "\n",
        "# Select the required columns\n",
        "result_df = big_countries_df.select(\"name\", \"population\", \"area\")\n",
        "\n",
        "# Show the result\n",
        "result_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NLyhFeqWYuzj",
        "outputId": "cde68cf3-0547-46ca-8e5d-c8e97e0ae208"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+----------+-------+\n",
            "|       name|population|   area|\n",
            "+-----------+----------+-------+\n",
            "|Afghanistan|  25500100| 652230|\n",
            "|    Algeria|  37100000|2381741|\n",
            "+-----------+----------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h23E-EQXZAK3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}